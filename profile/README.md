# Welcome to MMPATH: MultiModal Pipelines for Affective Tracking in Humans. 

This organization brings together tools developed in the Grosenick lab at Cornell for using modern AI/ML methods for facial posture tracking, utterance embedding, and vocal prosody representation to track affective state in one or more people recorded using video.

Currently we have made our pipeline for processessing video files to perform speaker diarization and extract combined multimodal features (acoustic, linguistic, and facial features) available in the MultimodalFeatureSync repository. See the README in that repo for usage and license. This code has been alpha tested and is ready to try!

Coming soon: in fall 2024 we will also openly release our integrated code for embedding the data streams produced by MultimodalFeatureSync for tracking affect as a latent variables in individuals and dyads!



<!--

**Here are some ideas to get you started:**

ðŸ™‹â€â™€ï¸ A short introduction - what is your organization all about?
ðŸŒˆ Contribution guidelines - how can the community get involved?
ðŸ‘©â€ðŸ’» Useful resources - where can the community find your docs? Is there anything else the community should know?
ðŸ¿ Fun facts - what does your team eat for breakfast?
ðŸ§™ Remember, you can do mighty things with the power of [Markdown](https://docs.github.com/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)
-->
